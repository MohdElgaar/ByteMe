{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawling class tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import files and other settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import django\n",
    "import os\n",
    "os.environ.setdefault(\"DJANGO_SETTINGS_MODULE\", \"byteme.settings.base\")\n",
    "django.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crawler.models import Crawler\n",
    "from accounts.models import Speaker, UserProfile, User\n",
    "from events.tag import Tag\n",
    "from events.models import Event\n",
    "from events.views import approveEventChange\n",
    "from time import sleep\n",
    "import csv\n",
    "import json\n",
    "import pickle\n",
    "import sys\n",
    "import random\n",
    "from datetime import datetime\n",
    "from django.utils import timezone\n",
    "\n",
    "sys.setrecursionlimit(100000)\n",
    "my_crawler = Crawler()\n",
    "my_crawler.verbose = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "univ_list = ['Kaist', 'Korea Advanced Institute of Science and Technology', 'Stanford University',\n",
    "       'Cambridge', 'MIT', 'Yale', 'Georgia Institute of Technology', 'Harvard University', 'ETH Zurich', 'EPFL', 'Oxford University',\n",
    "       'Imperial College London', 'NUS', 'NTU', 'Princeton', 'Cornell', 'Tshinghua']\n",
    "univ_scholar_ids = [[] for i in univ_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This failed\n",
    "for count, univ in enumerate(univ_list):\n",
    "    try:\n",
    "        ids = my_crawler.crawl_univ_scholar_ids(univ, 50)\n",
    "        univ_scholar_ids.append(ids)\n",
    "        print(count, univ, len(ids))\n",
    "    except Exception:\n",
    "        ids = []\n",
    "        print('Exception Occured')\n",
    "    univ_scholar_ids[count] = ids\n",
    "    sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ids = set()\n",
    "with open('scholars.csv', newline='\\n') as csvfile:\n",
    "    m_reader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "    for row in m_reader:\n",
    "        tag_id = row[1:]\n",
    "        all_ids = all_ids.union(set(tag_id))\n",
    "all_ids = list(all_ids)\n",
    "print(len(all_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scholar_dic_list = []\n",
    "for count, cur_id in enumerate(all_ids):\n",
    "    print(count, ': ', cur_id)\n",
    "    cur_dic = my_crawler.crawl_scholar(cur_id)\n",
    "    scholar_dic_list.append(cur_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('scholars.csv', 'w', newline='\\n') as csvfile:\n",
    "    m_writer = csv.writer(csvfile, delimiter=',',\n",
    "                            quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "    for ids, univ in zip(univ_scholar_ids, univ_list):\n",
    "        row = [i for i in ids]\n",
    "        row.insert(0, univ)\n",
    "        m_writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('crawling_output/scholar_crawled.pickle', 'wb') as handle:\n",
    "    pickle.dump(scholar_dic_list, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Scholar info, set-up users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('crawling_output/scholar_crawled.pickle', 'rb') as handle:\n",
    "    scholar_dic_pickle = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scholar_list = []\n",
    "for scholar_dic in scholar_dic_pickle:\n",
    "    name = scholar_dic['name']\n",
    "    try:\n",
    "        univ = scholar_dic['association']\n",
    "    except KeyError:\n",
    "        univ = 'Kaist'\n",
    "    email = ''.join(name.split(' ')) + str(random.randint(0,100)) + '@' + univ.split(' ')[0] + '.edu'\n",
    "    email = email.lower()\n",
    "    #print(name, univ, email)\n",
    "    scholar = Speaker.objects.create(name=name, univ=univ, speakerEmail=email)\n",
    "    if scholar_dic != {}:\n",
    "        if 'field_of_study' in scholar_dic.keys():\n",
    "            tag_objects = my_crawler.update_tag_info(scholar_dic['field_of_study'])\n",
    "            scholar_dic['tags'] = tag_objects\n",
    "        my_crawler.update_scholar_info(scholar, scholar_dic)\n",
    "    scholar_list.append(scholar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_urls = []\n",
    "with open('crawling_output/images.csv', newline='\\n') as csvfile:\n",
    "    m_reader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "    for row in m_reader:\n",
    "        image_urls.append(row[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[128, 0, 131, 136, 140, 141, 12, 14, 144, 17, 18, 147, 29, 160, 36, 165, 169, 42, 171, 45, 174, 47, 177, 50, 179, 52, 53, 56, 184, 58, 60, 192, 65, 69, 71, 75, 79, 81, 84, 87, 96, 100, 102, 106, 109, 111, 113, 120, 123, 124]\n"
     ]
    }
   ],
   "source": [
    "num_random_scholars = 50\n",
    "index_list = set()\n",
    "while len(index_list) != num_random_scholars:\n",
    "    index_list.add(random.randint(0, len(scholar_dic_pickle)-1))\n",
    "index_list = list(index_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<QuerySet [<User: admin>, <User: myaldiz>]> <QuerySet [<UserProfile: admin, KAIST>, <UserProfile: myaldiz, KAIST>]>\n"
     ]
    }
   ],
   "source": [
    "admin = User.objects.create_superuser(username='admin', password='password@', email='berk17@gmail.com')\n",
    "UserProfile.objects.create(user=admin)\n",
    "username = 'myaldiz'\n",
    "passw = '1234'\n",
    "email = 'myaldiz@kaist.ac.kr'\n",
    "staff = True\n",
    "django_user = User.objects.create_user(username, password=passw, email= email, is_staff=staff)\n",
    "user = UserProfile.objects.create(user=django_user)\n",
    "print(User.objects.all(), UserProfile.objects.all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('crawling_output/Events.json') as file:\n",
    "    vals = json.load(file)\n",
    "abstract_list = []\n",
    "place_list = []\n",
    "title_list = []\n",
    "details_list = []\n",
    "for i in vals['Events']:\n",
    "    abstract_list.append(i['abstract'])\n",
    "    place_list.append(i['place'])\n",
    "    title_list.append(i['title'])\n",
    "    details_list.append(i['details'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in index_list:\n",
    "    scholar_dic = scholar_dic_pickle[index]\n",
    "    scholar = scholar_list[index] \n",
    "    image_url = image_urls[random.randint(0, len(image_urls)-1)]\n",
    "    time = timezone.now()\n",
    "    time = time.replace(year=2019, month = random.randint(1,12), \n",
    "                        day=random.randint(1,25), hour=12, minute=0)\n",
    "    abstract = abstract_list[random.randint(0,len(abstract_list)-1)]\n",
    "    place = place_list[random.randint(0,len(place_list)-1)]\n",
    "    title = title_list[random.randint(0,len(title_list)-1)]\n",
    "    details = details_list[random.randint(0,len(details_list)-1)]\n",
    "    cur_event = Event.objects.create(creater = user, speaker = scholar, time = time, timeReq = time, speakerReq = scholar, \n",
    "                                     req = \"add\", placeReq=place, titleReq=title, detailsReq=details,\n",
    "                                     abstractReq=abstract, imgurLReq=image_url)\n",
    "    tags = set()\n",
    "    while (len(tags) != 10):\n",
    "        cur_tag = Tag.objects.all()[random.randint(0,len(Tag.objects.all())-1)]\n",
    "        tags.add(cur_tag)\n",
    "    for tag in tags:\n",
    "        cur_event.tags.add(tag)\n",
    "    \n",
    "    if bool(random.randint(0,2)):\n",
    "        approveEventChange(cur_event.identifier, req=\"add\")\n",
    "    cur_event.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Speaker Crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = Speaker(name='Min H Kim', speakerEmail='1@kaist.ac.kr')\n",
    "s2 = Speaker(name='Daniel Suk Jeon', speakerEmail='2@kaist.ac.kr')\n",
    "s3 = Speaker(name='Osman', univ='Stanford', speakerEmail='3@kaist.ac.kr')\n",
    "s1.save()\n",
    "s2.save()\n",
    "s3.save()\n",
    "print(Speaker.objects.all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = Speaker.objects.all()[0]\n",
    "s2 = Speaker.objects.all()[1]\n",
    "s3 = Speaker.objects.all()[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_crawler.scholar_crawl_request(s1)\n",
    "my_crawler.scholar_crawl_request(s2)\n",
    "my_crawler.scholar_crawl_request(s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Tag.objects.all())\n",
    "for speaker in Speaker.objects.all():\n",
    "    print(speaker.tags.all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indiv Score testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('UserProfiles: ', UserProfile.objects.all())\n",
    "print('Speakers: ', Speaker.objects.all())\n",
    "print('Tags:', Tag.objects.all())\n",
    "print('Events:', Event.objects.all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event = Event.objects.all()[0]\n",
    "user = UserProfile.objects.all()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = event.generateRankingScore(user)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event.speaker.citations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytrends Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytrends\n",
    "from pytrends.request import TrendReq\n",
    "\n",
    "pytrend = TrendReq(hl='en-US', tz=360)\n",
    "#pytrend = TrendReq(hl='en-US', tz=360, proxies = {'https': 'https://34.203.233.13:80'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kw_list = [\"Blockchain\"]\n",
    "kw_list = ['generative adverserial network', 'neural machine translation', 'neural turing machine']\n",
    "pytrend.build_payload(kw_list, cat=0, timeframe='today 5-y', geo='', gprop='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interest_over_time_df = pytrend.interest_over_time()\n",
    "print(interest_over_time_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_scholar_id(in_str):\n",
    "    idx1 = in_str.find('user=') + 5\n",
    "    idx2 = in_str.find('&') #Check this!!\n",
    "    substr = in_str[idx1:idx2]\n",
    "    return substr\n",
    "\n",
    "def create_link(scholar_id):\n",
    "    return \"http://scholar.google.com/citations?user=\" + scholar_id + \"&hl=en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Dieter Fox google scholar\"\n",
    "\n",
    "matches = set()\n",
    "for j in search(query, stop=5): \n",
    "    if \"scholar.google\" in j and len(j) < 100:\n",
    "        scholar_id = parse_scholar_id(j)\n",
    "        matches.add(scholar_id)\n",
    "        \n",
    "print('Number of links: ', len(matches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scholar_id = matches.pop()\n",
    "link = create_link(scholar_id)\n",
    "print(link)\n",
    "page = requests.get(link)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "print(soup.prettify())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
